{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a data_set matrix\n",
    "def create_data_set(_data_set, _look_back, _look_forward):\n",
    "    data_x, data_y = [], []\n",
    "    for i in range(len(_data_set) - _look_back - _look_forward):\n",
    "        a = _data_set[i:(i + _look_back)]\n",
    "        data_x.append(a)\n",
    "        data_y.append([_data_set[i + j + _look_back] for j in range(_look_forward)])\n",
    "    return np.array(data_x), np.array(data_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../Datapreprocessing/Data/Smoothing/Train_x_LB:10_s', 'rb') as fp:\n",
    "    train_lb10 = pickle.load(fp)\n",
    "with open ('../Datapreprocessing/Data/Smoothing/Val_x_LB:10_s', 'rb') as fp:\n",
    "    val_lb10 = pickle.load(fp)\n",
    "with open ('../Datapreprocessing/Data/Smoothing/Test_x_LB:10_s', 'rb') as fp:\n",
    "    test_lb10 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../Datapreprocessing/Data/Smoothing/Train_x_LB:100_25%_s', 'rb') as fp:\n",
    "    train_lb100 = pickle.load(fp)\n",
    "with open ('../Datapreprocessing/Data/Smoothing/Val_x_LB:100_25%_s', 'rb') as fp:\n",
    "    val_lb100 = pickle.load(fp)\n",
    "with open ('../Datapreprocessing/Data/Smoothing/Test_x_LB:100_25%_s', 'rb') as fp:\n",
    "    test_lb100 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../Datapreprocessing/Data/Smoothing/Train_x_LB:200_25%_s', 'rb') as fp:\n",
    "    train_lb200 = pickle.load(fp)\n",
    "with open ('../Datapreprocessing/Data/Smoothing/Val_x_LB:200_25%_s', 'rb') as fp:\n",
    "    val_lb200 = pickle.load(fp)\n",
    "with open ('../Datapreprocessing/Data/Smoothing/Test_x_LB:200_25%_s', 'rb') as fp:\n",
    "    test_lb200 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the train data_set\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_lb10 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in train_lb10]\n",
    "val_lb10 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in val_lb10]\n",
    "test_lb10 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in test_lb10]\n",
    "\n",
    "train_lb100 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in train_lb100]\n",
    "val_lb100 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in val_lb100]\n",
    "test_lb100 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in test_lb100]\n",
    "\n",
    "train_lb200 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in train_lb200]\n",
    "val_lb200 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in val_lb200]\n",
    "test_lb200 = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in test_lb200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 10\n",
    "look_forward = 10\n",
    "units = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add look back\n",
    "train_x = []\n",
    "train_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in train_lb10:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    train_x.append(tmp_x)\n",
    "    train_y.append(tmp_y)\n",
    "\n",
    "train_x_10 = np.array([n for m in train_x for n in m])   \n",
    "train_y_10 = np.array([n for m in train_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = []\n",
    "val_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in val_lb10:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    val_x.append(tmp_x)\n",
    "    val_y.append(tmp_y)\n",
    "    \n",
    "val_x_10 = np.array([n for m in val_x for n in m])\n",
    "val_y_10 = np.array([n for m in val_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in test_lb10:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    test_x.append(tmp_x)\n",
    "    test_y.append(tmp_y)\n",
    "    \n",
    "test_x_10 = np.array([n for m in test_x for n in m])\n",
    "test_y_10 = np.array([n for m in test_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "train_y_10 = np.reshape(train_y_10, (train_y_10.shape[0], look_forward))\n",
    "val_y_10 = np.reshape(val_y_10, (val_y_10.shape[0], look_forward))\n",
    "test_y_10 = np.reshape(test_y_10, (test_y_10.shape[0], look_forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 679481 samples, validate on 84439 samples\n",
      "Epoch 1/50\n",
      "679481/679481 [==============================] - 70s 103us/step - loss: 0.0207 - val_loss: 0.0152\n",
      "Epoch 2/50\n",
      "679481/679481 [==============================] - 69s 102us/step - loss: 0.0191 - val_loss: 0.0142\n",
      "Epoch 3/50\n",
      "679481/679481 [==============================] - 69s 102us/step - loss: 0.0186 - val_loss: 0.0119\n",
      "Epoch 4/50\n",
      "679481/679481 [==============================] - 72s 106us/step - loss: 0.0185 - val_loss: 0.0128\n",
      "Epoch 5/50\n",
      "679481/679481 [==============================] - 81s 120us/step - loss: 0.0184 - val_loss: 0.0124\n",
      "Epoch 6/50\n",
      "679481/679481 [==============================] - 80s 118us/step - loss: 0.0183 - val_loss: 0.0118\n",
      "Epoch 7/50\n",
      "679481/679481 [==============================] - 80s 118us/step - loss: 0.0183 - val_loss: 0.0137\n",
      "Epoch 8/50\n",
      "679481/679481 [==============================] - 80s 118us/step - loss: 0.0182 - val_loss: 0.0128\n",
      "Epoch 9/50\n",
      "679481/679481 [==============================] - 84s 123us/step - loss: 0.0181 - val_loss: 0.0121\n",
      "Epoch 10/50\n",
      "679481/679481 [==============================] - 81s 119us/step - loss: 0.0181 - val_loss: 0.0116\n",
      "Epoch 11/50\n",
      "679481/679481 [==============================] - 94s 139us/step - loss: 0.0181 - val_loss: 0.0129\n",
      "Epoch 12/50\n",
      "679481/679481 [==============================] - 108s 159us/step - loss: 0.0180 - val_loss: 0.0123\n",
      "Epoch 13/50\n",
      "679481/679481 [==============================] - 73s 107us/step - loss: 0.0180 - val_loss: 0.0129\n",
      "Epoch 14/50\n",
      "679481/679481 [==============================] - 66s 97us/step - loss: 0.0180 - val_loss: 0.0126\n",
      "Epoch 15/50\n",
      "679481/679481 [==============================] - 66s 97us/step - loss: 0.0179 - val_loss: 0.0124\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(look_back, 1)))\n",
    "model.add(Dense(look_forward))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(train_x_10, train_y_10, epochs=50, batch_size=256, verbose=1, callbacks = callback, validation_data=(val_x_10, val_y_10))\n",
    "model.save('lb{}_lf{}_u{}_full_s.h5'.format(look_back, look_forward, units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add look back\n",
    "train_x = []\n",
    "train_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in train_lb100:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    train_x.append(tmp_x)\n",
    "    train_y.append(tmp_y)\n",
    "\n",
    "train_x_100 = np.array([n for m in train_x for n in m])   \n",
    "train_y_100 = np.array([n for m in train_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = []\n",
    "val_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in val_lb100:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    val_x.append(tmp_x)\n",
    "    val_y.append(tmp_y)\n",
    "    \n",
    "val_x_100 = np.array([n for m in val_x for n in m])\n",
    "val_y_100 = np.array([n for m in val_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in test_lb100:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    test_x.append(tmp_x)\n",
    "    test_y.append(tmp_y)\n",
    "    \n",
    "test_x_100 = np.array([n for m in test_x for n in m])\n",
    "test_y_100 = np.array([n for m in test_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "train_y_100 = np.reshape(train_y_100, (train_y_100.shape[0], look_forward))\n",
    "val_y_100 = np.reshape(val_y_100, (val_y_100.shape[0], look_forward))\n",
    "test_y_100 = np.reshape(test_y_100, (test_y_100.shape[0], look_forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113258 samples, validate on 18598 samples\n",
      "Epoch 1/50\n",
      "113258/113258 [==============================] - 58s 508us/step - loss: 0.0114 - val_loss: 0.0081\n",
      "Epoch 2/50\n",
      "113258/113258 [==============================] - 57s 505us/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 3/50\n",
      "113258/113258 [==============================] - 57s 504us/step - loss: 0.0054 - val_loss: 0.0063\n",
      "Epoch 4/50\n",
      "113258/113258 [==============================] - 57s 503us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 5/50\n",
      "113258/113258 [==============================] - 57s 503us/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 6/50\n",
      "113258/113258 [==============================] - 58s 512us/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 7/50\n",
      "113258/113258 [==============================] - 58s 511us/step - loss: 0.0053 - val_loss: 0.0055\n",
      "Epoch 8/50\n",
      "113258/113258 [==============================] - 59s 523us/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 9/50\n",
      "113258/113258 [==============================] - 59s 520us/step - loss: 0.0053 - val_loss: 0.0053\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(look_back, 1)))\n",
    "model.add(Dense(look_forward))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(train_x_100, train_y_100, epochs=50, batch_size=256, verbose=1, callbacks = callback, validation_data=(val_x_100, val_y_100))\n",
    "model.save('lb{}_lf{}_u{}_s.h5'.format(look_back, look_forward, units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add look back\n",
    "train_x = []\n",
    "train_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in train_lb200:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    train_x.append(tmp_x)\n",
    "    train_y.append(tmp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_200 = np.array([n for m in train_x for n in m])   \n",
    "train_y_200 = np.array([n for m in train_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = []\n",
    "val_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in val_lb200:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    val_x.append(tmp_x)\n",
    "    val_y.append(tmp_y)\n",
    "    \n",
    "val_x_200 = np.array([n for m in val_x for n in m])\n",
    "val_y_200 = np.array([n for m in val_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "# reshape into X=t and Y=t+n\n",
    "for sentence in test_lb200:\n",
    "    tmp_x, tmp_y = create_data_set(sentence, look_back, look_forward)\n",
    "    test_x.append(tmp_x)\n",
    "    test_y.append(tmp_y)\n",
    "    \n",
    "test_x_200 = np.array([n for m in test_x for n in m])\n",
    "test_y_200 = np.array([n for m in test_y for n in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "train_y_200 = np.reshape(train_y_200, (train_y_200.shape[0], look_forward))\n",
    "val_y_200 = np.reshape(val_y_200, (val_y_200.shape[0], look_forward))\n",
    "test_y_200 = np.reshape(test_y_200, (test_y_200.shape[0], look_forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67821 samples, validate on 18598 samples\n",
      "Epoch 1/50\n",
      "67821/67821 [==============================] - 74s 1ms/step - loss: 0.0144 - val_loss: 0.0110\n",
      "Epoch 2/50\n",
      "67821/67821 [==============================] - 70s 1ms/step - loss: 0.0068 - val_loss: 0.0087\n",
      "Epoch 3/50\n",
      "67821/67821 [==============================] - 70s 1ms/step - loss: 0.0051 - val_loss: 0.0063\n",
      "Epoch 4/50\n",
      "67821/67821 [==============================] - 71s 1ms/step - loss: 0.0047 - val_loss: 0.0065\n",
      "Epoch 5/50\n",
      "67821/67821 [==============================] - 71s 1ms/step - loss: 0.0047 - val_loss: 0.0063\n",
      "Epoch 6/50\n",
      "67821/67821 [==============================] - 71s 1ms/step - loss: 0.0046 - val_loss: 0.0059\n",
      "Epoch 7/50\n",
      "67821/67821 [==============================] - 71s 1ms/step - loss: 0.0046 - val_loss: 0.0059\n",
      "Epoch 8/50\n",
      "67821/67821 [==============================] - 71s 1ms/step - loss: 0.0046 - val_loss: 0.0065\n",
      "Epoch 9/50\n",
      "67821/67821 [==============================] - 71s 1ms/step - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 10/50\n",
      "67821/67821 [==============================] - 71s 1ms/step - loss: 0.0046 - val_loss: 0.0058\n",
      "Epoch 11/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 12/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 13/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0057\n",
      "Epoch 14/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 15/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0060\n",
      "Epoch 16/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 17/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 18/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 19/50\n",
      "67821/67821 [==============================] - 73s 1ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 20/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 21/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0058\n",
      "Epoch 22/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 23/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 24/50\n",
      "67821/67821 [==============================] - 72s 1ms/step - loss: 0.0046 - val_loss: 0.0052\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(look_back, 1)))\n",
    "model.add(Dense(look_forward))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(train_x_200, train_y_200, epochs=50, batch_size=256, verbose=1, callbacks = callback,validation_data=(val_x_200, val_y_200))\n",
    "model.save('lb{}_lf{}_u{}_s.h5'.format(look_back, look_forward, units))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
