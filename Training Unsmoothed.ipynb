{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 200\n",
    "look_forward = 10\n",
    "units = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Datapreprocessing/Data/No Smoothing/Train_x_LB:{}'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    train = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/No Smoothing/Test_x_LB:{}'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    test = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/No Smoothing/Val_x_LB:{}'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    val = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/No Smoothing/Train_y_LB:{}'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    dctr_y = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/No Smoothing/Test_y_LB:{}'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    dcte_y = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/No Smoothing/Val_y_LB:{}'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    dcva_y = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data_set\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in train if l != []]\n",
    "test = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in test if l != []]\n",
    "val = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in val if l != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a data_set matrix\n",
    "def create_data_set(_data_set, _look_back=1, look_forward=10):\n",
    "    data_x, data_y = [], []\n",
    "    for i in range(len(_data_set) - _look_back - look_forward):\n",
    "        a = _data_set[i:(i + _look_back)]\n",
    "        data_x.append(a)\n",
    "        data_y.append([_data_set[i + j + _look_back] for j in range(look_forward)])\n",
    "    return np.array(data_x), np.array(data_y) \n",
    "\n",
    "def remove_speaker(data, labels, speaker=0):\n",
    "    tmp_x = []\n",
    "    tmp_y = []\n",
    "    # reshape into X=t and Y=t+n\n",
    "    for i in range(len(data)):\n",
    "        if labels[i][speaker] != 1: \n",
    "            sentence = data[i]\n",
    "            x, y = create_data_set(sentence, look_back, look_forward)\n",
    "            tmp_x.append(x)\n",
    "            tmp_y.append(y)\n",
    "    tmp_x = np.array([n for m in tmp_x for n in m], dtype='float16')\n",
    "    tmp_y = np.array([n for m in tmp_y for n in m], dtype='float16')\n",
    "    s = tmp_x.shape\n",
    "#     return tmp_x.reshape(s[0], look_back), tmp_y.reshape(s[0], look_forward)\n",
    "    return tmp_x, tmp_y.reshape(s[0], look_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = remove_speaker(train, dctr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y = remove_speaker(val, dcva_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = remove_speaker(test, dcte_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(653877, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 653877 samples, validate on 85285 samples\n",
      "Epoch 1/50\n",
      "653877/653877 [==============================] - 3095s 5ms/step - loss: 0.0748 - val_loss: 0.0732\n",
      "Epoch 2/50\n",
      "653877/653877 [==============================] - 3132s 5ms/step - loss: 0.0711 - val_loss: 0.0727\n",
      "Epoch 3/50\n",
      "653877/653877 [==============================] - 3837s 6ms/step - loss: 0.0702 - val_loss: 0.0718\n",
      "Epoch 4/50\n",
      "653877/653877 [==============================] - 2877s 4ms/step - loss: 0.0693 - val_loss: 0.0714\n",
      "Epoch 5/50\n",
      "653877/653877 [==============================] - 2899s 4ms/step - loss: 0.0684 - val_loss: 0.0704\n",
      "Epoch 6/50\n",
      "653877/653877 [==============================] - 2900s 4ms/step - loss: 0.0676 - val_loss: 0.0701\n",
      "Epoch 7/50\n",
      "653877/653877 [==============================] - 2889s 4ms/step - loss: 0.0669 - val_loss: 0.0694\n",
      "Epoch 8/50\n",
      "653877/653877 [==============================] - 2879s 4ms/step - loss: 0.0661 - val_loss: 0.0697\n",
      "Epoch 9/50\n",
      "653877/653877 [==============================] - 2879s 4ms/step - loss: 0.0653 - val_loss: 0.0707\n",
      "Epoch 10/50\n",
      "653877/653877 [==============================] - 2876s 4ms/step - loss: 0.0644 - val_loss: 0.0704\n",
      "Epoch 11/50\n",
      "653877/653877 [==============================] - 2880s 4ms/step - loss: 0.0635 - val_loss: 0.0713\n",
      "Epoch 12/50\n",
      "653877/653877 [==============================] - 2879s 4ms/step - loss: 0.0624 - val_loss: 0.0725\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "patience = 5\n",
    "callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto')]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(look_back, 1)))\n",
    "# model.add(LSTM(units, input_shape=(look_back, 1)))\n",
    "model.add(Dense(look_forward))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(train_x, train_y, epochs=50, batch_size=256, verbose=1,  callbacks = callback, validation_data=(val_x, val_y))\n",
    "model.save('lb{}_lf{}_u{}_full_p{}_ts.h5'.format(look_back, look_forward, units, patience))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 100\n",
    "look_forward = 10\n",
    "units = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Datapreprocessing/Data/Smoothing/Train_x_LB:{}_s'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    train = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/Smoothing/Test_x_LB:{}_s'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    test = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/Smoothing/Val_x_LB:{}_s'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    val = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/Smoothing/Train_y_LB:{}_s'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    dctr_y = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/Smoothing/Test_y_LB:{}_s'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    dcte_y = pickle.load(filehandle)\n",
    "with open('../Datapreprocessing/Data/Smoothing/Val_y_LB:{}_s'.format(look_back), 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    dcva_y = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data_set\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in train if l != []]\n",
    "test = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in test if l != []]\n",
    "val = [scaler.fit_transform(np.array(l).reshape(-1,1)) for l in val if l != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = remove_speaker(train, dctr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y = remove_speaker(val, dcva_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = remove_speaker(test, dcte_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284934, 100, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 284934 samples, validate on 52502 samples\n",
      "Epoch 1/50\n",
      "284934/284934 [==============================] - 313s 1ms/step - loss: 0.0198 - val_loss: 0.0259\n",
      "Epoch 2/50\n",
      "284934/284934 [==============================] - 321s 1ms/step - loss: 0.0172 - val_loss: 0.0257\n",
      "Epoch 3/50\n",
      "284934/284934 [==============================] - 310s 1ms/step - loss: 0.0171 - val_loss: 0.0175\n",
      "Epoch 4/50\n",
      "284934/284934 [==============================] - 301s 1ms/step - loss: 0.0170 - val_loss: 0.0196\n",
      "Epoch 5/50\n",
      "284934/284934 [==============================] - 298s 1ms/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 6/50\n",
      "284934/284934 [==============================] - 293s 1ms/step - loss: 0.0168 - val_loss: 0.0199\n",
      "Epoch 7/50\n",
      "284934/284934 [==============================] - 295s 1ms/step - loss: 0.0167 - val_loss: 0.0183\n",
      "Epoch 8/50\n",
      "284934/284934 [==============================] - 314s 1ms/step - loss: 0.0166 - val_loss: 0.0182\n",
      "Epoch 9/50\n",
      "284934/284934 [==============================] - 283s 993us/step - loss: 0.0165 - val_loss: 0.0215\n",
      "Epoch 10/50\n",
      "284934/284934 [==============================] - 283s 993us/step - loss: 0.0165 - val_loss: 0.0189\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "patience = 5\n",
    "callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto')]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(look_back, 1)))\n",
    "# model.add(LSTM(units, input_shape=(look_back, 1)))\n",
    "model.add(Dense(look_forward))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(train_x, train_y, epochs=50, batch_size=256, verbose=1,  callbacks = callback, validation_data=(val_x, val_y))\n",
    "model.save('lb{}_lf{}_u{}_full_p{}_bs_s.h5'.format(look_back, look_forward, units, patience))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
